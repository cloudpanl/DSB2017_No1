{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prototype inference demo\n",
    "Each step can be run separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Import and read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hasee\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dicom\\__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import full_prep\n",
    "from config_submit import config as config_submit\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from layers import acc\n",
    "from data_detector import DataBowl3Detector,collate\n",
    "from data_classifier import DataBowl3Classifier\n",
    "\n",
    "from utils import *\n",
    "from split_combine import SplitComb\n",
    "from test_detect import test_detect\n",
    "from importlib import import_module\n",
    "import pandas\n",
    "\n",
    "datapath = config_submit['datapath']\n",
    "prep_result_path = config_submit['preprocess_result_path']\n",
    "skip_prep = config_submit['skip_preprocessing']\n",
    "skip_detect = config_submit['skip_detect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Pre-process stage1 / 2 data for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import full_prep\n",
    "from config_submit import config as config_submit\n",
    "\n",
    "datapath = config_submit['datapath']\n",
    "prep_result_path = config_submit['preprocess_result_path']\n",
    "skip_prep = config_submit['skip_preprocessing']\n",
    "skip_detect = config_submit['skip_detect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting preprocessing\n",
      "end preprocessing\n"
     ]
    }
   ],
   "source": [
    "print('datapath:', datapath)\n",
    "prep_result_path_demo = 'F:\\\\LargeFiles\\\\lfz\\\\prep_result_demo\\\\'\n",
    "print('prep_result_path_demo:', prep_result_path_demo)\n",
    "\n",
    "testsplit = full_prep(datapath,prep_result_path_demo,\n",
    "                    n_worker = 1, # config_submit['n_worker_preprocessing']\n",
    "                    use_existing=False) # config_submit['use_exsiting_preprocessing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Load N-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hasee\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dicom\\__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import full_prep\n",
    "from config_submit import config as config_submit\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from layers import acc\n",
    "from data_detector import DataBowl3Detector,collate\n",
    "from data_classifier import DataBowl3Classifier\n",
    "\n",
    "from utils import *\n",
    "from split_combine import SplitComb\n",
    "from test_detect import test_detect\n",
    "from importlib import import_module\n",
    "import pandas\n",
    "\n",
    "datapath = config_submit['datapath']\n",
    "prep_result_path = config_submit['preprocess_result_path']\n",
    "skip_prep = config_submit['skip_preprocessing']\n",
    "skip_detect = config_submit['skip_detect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodmodel = import_module(config_submit['detector_model'].split('.py')[0])\n",
    "config1, nod_net, loss, get_pbb = nodmodel.get_model()\n",
    "checkpoint = torch.load(config_submit['detector_param'])\n",
    "nod_net.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nod_net= nod_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(nod_net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip prep\n",
    "Load the preprocessed npy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0015ceb851d7251b8f399e39779d1e7d', '0030a160d58723ff36d73f41b170ec21']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsplit = os.listdir(datapath)\n",
    "testsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_result_path = './bbox_result'\n",
    "if not os.path.exists(bbox_result_path):\n",
    "    os.mkdir(bbox_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 16 # TODO 32\n",
    "sidelen = 64 # TODO 144\n",
    "config1['datadir'] = prep_result_path\n",
    "split_comber = SplitComb(sidelen,config1['max_stride'],config1['stride'],margin,pad_value= config1['pad_value'])\n",
    "\n",
    "dataset = DataBowl3Detector(testsplit,config1,phase='test',split_comber=split_comber)\n",
    "test_loader = DataLoader(dataset,batch_size = 1,\n",
    "    shuffle = False,num_workers = 4,pin_memory=False,collate_fn =collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 3, 24, 24, 24])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 96, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\SW_WS\\github_SW\\DSB2017-lfz-pytorch\\test_detect.py:46: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  input = Variable(data[splitlist[i]:splitlist[i+1]], volatile = True) # TODO: no_grad() .cuda()\n",
      "E:\\SW_WS\\github_SW\\DSB2017-lfz-pytorch\\test_detect.py:47: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputcoord = Variable(coord[splitlist[i]:splitlist[i+1]], volatile = True) # TODO .cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'F:\\\\LargeFiles\\\\lfz\\\\prep_result_sub\\\\0015ceb851d7251b8f399e39779d1e7d']\n",
      "torch.Size([100, 1, 96, 96, 96])\n",
      "[1, 'F:\\\\LargeFiles\\\\lfz\\\\prep_result_sub\\\\0030a160d58723ff36d73f41b170ec21']\n",
      "elapsed time is 858.38 seconds\n"
     ]
    }
   ],
   "source": [
    "test_detect(test_loader, nod_net, get_pbb, bbox_result_path,config1,n_gpu=config_submit['n_gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3 :Load C-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_submit import config as config_submit\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from data_detector import DataBowl3Detector,collate\n",
    "from importlib import import_module\n",
    "import os\n",
    "from data_classifier import DataBowl3Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "casemodel = import_module(config_submit['classifier_model'].split('.py')[0])\n",
    "casenet = casemodel.CaseNet(topk=5)\n",
    "checkpoint = torch.load(config_submit['classifier_param'])\n",
    "casenet.load_state_dict(checkpoint['state_dict'])\n",
    "casenet = casenet.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0015ceb851d7251b8f399e39779d1e7d', '0030a160d58723ff36d73f41b170ec21']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = config_submit['datapath']\n",
    "testsplit = os.listdir(datapath)\n",
    "testsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = casemodel.config\n",
    "bbox_result_path = './bbox_result'\n",
    "if not os.path.exists(bbox_result_path):\n",
    "    os.mkdir(bbox_result_path)\n",
    "\n",
    "prep_result_path = config_submit['preprocess_result_path']\n",
    "config2['bboxpath'] = bbox_result_path\n",
    "config2['datadir'] = prep_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = config_submit['outputfile']\n",
    "\n",
    "def test_casenet(model,testset):\n",
    "    data_loader = DataLoader(\n",
    "        testset,\n",
    "        batch_size = 1,\n",
    "        shuffle = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory=True)\n",
    "    #model = model.cuda()\n",
    "    model.eval()\n",
    "    predlist = []\n",
    "\n",
    "    #     weight = torch.from_numpy(np.ones_like(y).float().cuda()\n",
    "    for i,(x,coord) in enumerate(data_loader):\n",
    "\n",
    "        coord = Variable(coord) # TODO .cuda()\n",
    "        x = Variable(x) # TODO .cuda()\n",
    "        nodulePred,casePred,_ = model(x,coord)\n",
    "        predlist.append(casePred.data.cpu().numpy())\n",
    "        #print([i,data_loader.dataset.split[i,1],casePred.data.cpu().numpy()])\n",
    "    predlist = np.concatenate(predlist)\n",
    "    return predlist    \n",
    "config2['bboxpath'] = bbox_result_path\n",
    "config2['datadir'] = prep_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataBowl3Classifier(testsplit, config2, phase = 'test')\n",
    "predlist = test_casenet(casenet,dataset).T\n",
    "df = pandas.DataFrame({'id':testsplit, 'cancer':predlist})\n",
    "df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataBowl3Classifier` in `data_classifier.py`\n",
    "Get top 5 for 'test' or sample 5 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = config_submit['datapath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0015ceb851d7251b8f399e39779d1e7d', '0030a160d58723ff36d73f41b170ec21']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "testsplit = os.listdir(datapath)\n",
    "testsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_classifier import DataBowl3Classifier\n",
    "\n",
    "dataset = DataBowl3Classifier(testsplit, config2, phase = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coord\n",
    "* 5, - top 5\n",
    "* 3, - \n",
    "* 16, \n",
    "* 16,\n",
    "* 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 16, 16, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3312)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0[1][0,:,:,:,:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = Variable(coord) # TODO .cuda()\n",
    "x = Variable(x) # TODO .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (preBlock): Sequential(\n",
       "    (0): Conv3d(1, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "  )\n",
       "  (forw1): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(24, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv3d(24, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (forw2): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (forw3): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (forw4): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (back2): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(131, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv3d(131, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): PostRes(\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (back3): Sequential(\n",
       "    (0): PostRes(\n",
       "      (conv1): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): PostRes(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (maxpool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (maxpool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (maxpool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (maxpool4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unmaxpool1): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
       "  (unmaxpool2): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
       "  (path1): Sequential(\n",
       "    (0): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (path2): Sequential(\n",
       "    (0): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (drop): Dropout3d(p=0.2)\n",
       "  (output): Sequential(\n",
       "    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(64, 15, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#         xlist: n x k x 1x 96 x 96 x 96\n",
    "#         coordlist: n x k x 3 x 24 x 24 x 24\n",
    "xsize = xlist.size()\n",
    "corrdsize = coordlist.size()\n",
    "# Reshape the tensor to remove the 3rd axis.\n",
    "xlist = xlist.view(-1,xsize[2],xsize[3],xsize[4],xsize[5])\n",
    "coordlist = coordlist.view(-1,corrdsize[2],corrdsize[3],corrdsize[4],corrdsize[5])\n",
    "casenet.NoduleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\LargeFiles\\\\lfz\\\\prep_result_sub\\\\'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0015ceb851d7251b8f399e39779d1e7d'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.split( './sss/0015ceb851d7251b8f399e39779d1e7d')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
